---
title: "Data Mining for BI Supervised Learning Project"
output:
  pdf_document: default
  word_document: default
---
# DATA MINING FOR BI
## Supervised Learning Project

### Loading libraries
```{r 1}
library(RCurl)
library(e1071)
#install.packages("caret")
library(caret)
# install.packages("doSNOW")
library(doSNOW)
library(ipred)
# install.packages("xgboost")
library(xgboost)
#library(devtools)
#devtools::install_github('topepo/caret/pkg/caret')
```
## Importing training set features using 'getURL' method and excluding the 4th col (i.e. week start date)
```{r 2}
trfeat <- getURL("https://s3.amazonaws.com/drivendata/data/44/public/dengue_features_train.csv")
trfeat <-read.csv(text = trfeat)
tr <- trfeat[, -c(4)]
```

## Importing training set labels: i.e., number of dengue cases by the same week, year, and city as in the training set features dataset,
```{r 3}
trlabel <- getURL("https://s3.amazonaws.com/drivendata/data/44/public/dengue_labels_train.csv")
trlabel <- read.csv(text = trlabel)
```

## Merging the training set features and lablels by city, year, and week of year
```{r 4}
trfinal <- merge(tr, trlabel, by=c("city", "year", "weekofyear"))
train <- trfinal
str(trfinal)
```

## grouping all features and dummy coding of features
```{r 5}
train$city <- as.factor(train$city)
features <- c("city","year","weekofyear","ndvi_ne","ndvi_nw","ndvi_se",  
"ndvi_sw", "precipitation_amt_mm", "reanalysis_air_temp_k", "reanalysis_avg_temp_k","reanalysis_dew_point_temp_k", "reanalysis_max_air_temp_k",  "reanalysis_min_air_temp_k", "reanalysis_precip_amt_kg_per_m2", "reanalysis_relative_humidity_percent", "reanalysis_sat_precip_amt_mm",         
"reanalysis_specific_humidity_g_per_kg", "reanalysis_tdtr_k",                  "station_avg_temp_c", "station_diur_temp_rng_c", "station_max_temp_c" ,  "station_min_temp_c", "station_precip_mm","total_cases")
train <- train[, features]
dummy.vars <- dummyVars(~ ., data = train[, -c(1:3,24)])
train.dummy <- predict(dummy.vars, train[, -c(1:3,24)])
```

## Imputation of missing values using bag imputation method
```{r 6}
pre.process <- preProcess(train.dummy, method = "bagImpute")
imputed.data <- predict(pre.process, train.dummy) 

train$ndvi_ne <- imputed.data[,1]
train$ndvi_nw <- imputed.data[,2]
train$ndvi_se <- imputed.data[,3]
train$ndvi_sw <- imputed.data[,4]
train$precipitation_amt_mm <- imputed.data[,5]
train$reanalysis_air_temp_k <- imputed.data[, 6]
train$reanalysis_avg_temp_k <- imputed.data[,7]
train$reanalysis_dew_point_temp_k <- imputed.data[,8]
train$reanalysis_max_air_temp_k <- imputed.data[,9]
train$reanalysis_min_air_temp_k <- imputed.data[,10]
train$reanalysis_precip_amt_kg_per_m2 <- imputed.data[,11]
train$reanalysis_relative_humidity_percent <- imputed.data[,12]
train$reanalysis_sat_precip_amt_mm <- imputed.data[,13]
train$reanalysis_specific_humidity_g_per_kg <- imputed.data[,14]
train$reanalysis_tdtr_k <- imputed.data[,15]
train$station_avg_temp_c <- imputed.data[,16]
train$station_diur_temp_rng_c <- imputed.data[,17]
train$station_max_temp_c <- imputed.data[,18]
train$station_min_temp_c <- imputed.data[,19]
train$station_precip_mm <- imputed.data[,20]
```
### Checking any missing values in imputed data
```{r 7}
anyNA(train)
```

### Splitting the training set 80:20, and checking the dimensions of split sets
```{r 8}
set.seed(54321)
indexes <- createDataPartition(train$total_cases, times = 1, p = .8, list = FALSE)
deng.train <- train[indexes,]
deng.test <- train[-indexes,]
dim(deng.test)
dim(deng.train)
```

### Defining the training control using repeated cross validation
```{r 9}
train.control <- trainControl(method = "repeatedcv",
                              number = 5,
                              repeats = 20,
                              search = "grid")
```


### Defining the tuning grid
```{r 10}
tune.grid <- expand.grid(eta = c(0.05, 0.06, 0.1),
                         nrounds = c(30, 60, 70),
                         max_depth = 3:8,
                         min_child_weight = c(2.0, 2.25, 2.5, 10),
                         colsample_bytree = c(0.3, 0.4, 0.5),
                         gamma = c(0, 0.01, 0.001),
                         subsample = 1)
```

### Registering clusters on RAM to improve computer performance while running the ML algorithm
```{r 11}
cl <- makeCluster(3, type = "SOCK")
registerDoSNOW(cl)
```
### Subsetting the traiing data for selective predictors in the subset, based on regression tree analysis performed separately for determing the significant contributers
```{r 12}
train2 = train[, c(1, 3, 4, 5, 6, 7, 11, 13, 17, 18, 24)]
```

### Obtaining the regressor from training dataset using the tuning grid and train controls and selective predictors from decision tree analysis
```{r 13}
caret.cv <- train(total_cases ~ .,
                  data = train2,
                  method = "xgbTree",
                  tuneGrid = tune.grid,
                  trControl = train.control)
```

### Stopping the clusters started earlier to free RAM
```{r 14}
stopCluster(cl)
```
### Predicting 'total_cases' (rounded to nearest 0 place of decimal) on split test-set using the regressor from split train-set
```{r 15}
preds <- predict(caret.cv, deng.test)
totalpredcasestrtest <- round(preds)
```

### Plotting predicted and acutal total_cases
```{r 16}
par(mfrow=c(2,1))
plot(totalpredcasestrtest)
plot(deng.test$total_cases)
```

### Checking mean absolute error (MAE)
```{r}
actual = deng.test$total_cases
predicted = totalpredcasestrtest
mae <- function(error)
{
  mean(abs(error))
}
error <- (actual-predicted)
mae(error)
```
### Importing the test set from the host site
```{r}
testset <- getURL("https://s3.amazonaws.com/drivendata/data/44/public/dengue_features_test.csv")
testset <- read.csv(text=testset)
ts <- testset[, -c(4)]
names(ts)
ts$total_cases <- NA
```

### Imputing missing values for the test set
```{r}
ts <- ts[, features]
ts$city <- as.factor(ts$city)
ts$weekofyear <- as.numeric(ts$weekofyear)

tsdummy.vars <- dummyVars(~ ., data = ts[, -c(1:3,24)])
ts.dummy <- predict(tsdummy.vars, ts[, -c(1:3,24)])

tspre.process <- preProcess(ts.dummy, method = "bagImpute")
tsimputed.data <- predict(tspre.process, ts.dummy)     

ts$ndvi_ne <- tsimputed.data[,1]
ts$ndvi_nw <- tsimputed.data[,2]
ts$ndvi_se <- tsimputed.data[,3]
ts$ndvi_sw <- tsimputed.data[,4]
ts$precipitation_amt_mm <- tsimputed.data[,5]
ts$reanalysis_air_temp_k <- tsimputed.data[, 6]
ts$reanalysis_avg_temp_k <- tsimputed.data[,7]
ts$reanalysis_dew_point_temp_k <- tsimputed.data[,8]
ts$reanalysis_max_air_temp_k <- tsimputed.data[,9]
ts$reanalysis_min_air_temp_k <- tsimputed.data[,10]
ts$reanalysis_precip_amt_kg_per_m2 <- tsimputed.data[,11]
ts$reanalysis_relative_humidity_percent <- tsimputed.data[,12]
ts$reanalysis_sat_precip_amt_mm <- tsimputed.data[,13]
ts$reanalysis_specific_humidity_g_per_kg <- tsimputed.data[,14]
ts$reanalysis_tdtr_k <- tsimputed.data[,15]
ts$station_avg_temp_c <- tsimputed.data[,16]
ts$station_diur_temp_rng_c <- tsimputed.data[,17]
ts$station_max_temp_c <- tsimputed.data[,18]
ts$station_min_temp_c <- tsimputed.data[,19]
ts$station_precip_mm <- tsimputed.data[,20]
```

### Predicting the 'total_cases' for the test set using the regressor from the training set
```{r}
ts$total_cases <- round(predict(caret.cv, ts))

```

### Plotting the time-series for the total_cases in the test set
```{r}
par(mfrow=c(1,1))
plot(ts$total_cases)
```

### Downloading the submission form 
```{r}
Submitformat <- getURL("https://s3.amazonaws.com/drivendata/data/44/public/submission_format.csv")
submitformat2 <- read.csv(text=Submitformat)
```

### Entering the predicted 'total_cases' from the test-set into the submission form
```{r}
submitformat2$total_cases<- ts$total_cases
```

### Exporting the submission form to local drive for uploading to the competition site
```{r}
write.csv(submitformat2, "D://STUDY//MSIS//DM//submit031920xgb_send.csv", row.names = FALSE)
```


